{
 "cells": [
  {
   "cell_type": "code",
   "source": "from transformers import BertModel, BertTokenizer\nimport nltk\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport random\nfrom scipy.stats import norm\nimport math\n",
   "metadata": {
    "cell_id": "2e6f54123347476091411c3638774e8b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c44167cd",
    "execution_start": 1650504017094,
    "execution_millis": 5231,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 298
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "# Load tab delimited text files into dataframes\n#train1 = pd.read_table(\"train.txt\")\n#dev = pd.read_table(\"dev.txt\")\n#test = pd.read_table(\"test.txt\")",
   "metadata": {
    "cell_id": "0329d0effc794e12839d956d374a3893",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b3ef0fd1",
    "execution_start": 1650504022328,
    "execution_millis": 3,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135,
    "deepnote_output_heights": [
     232.34375
    ]
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "# train_x, train_y = train1.iloc[:, 2].tolist(), train1.iloc[:, 1]\n# dev_x, dev_y = dev.iloc[:, 2].tolist(), dev.iloc[:, 1]\n# test_x, test_y = test.iloc[:, 2].tolist(), test.iloc[:, 1]\n# labels = {'Primary': 0, 'Secondary': 1, 'Tertiary': 2}",
   "metadata": {
    "cell_id": "63ab217aceff4c2daa5c86cc6e80d93f",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "551d60cc",
    "execution_start": 1650504022332,
    "execution_millis": 5,
    "owner_user_id": "3eca5ab8-c1d8-4e7e-a5c9-1240a0171db1",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Running on {}\".format(device))",
   "metadata": {
    "cell_id": "35c04f6b7c154ec7a74481cd10cd1911",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a5c9748a",
    "execution_start": 1650504022342,
    "execution_millis": 3,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 130
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Running on cpu\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "def read_labels(filename):\n    labels={}\n    with open(filename) as file:\n        for line in file:\n            cols = line.split(\"\\t\")\n            label = cols[1]\n            if label not in labels:\n                labels[label]=len(labels)\n    return labels",
   "metadata": {
    "cell_id": "d64499b78aa04779b726d61d6070db80",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e495c78e",
    "execution_start": 1650504022347,
    "execution_millis": 37,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 225
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "def read_data(filename, labels, max_data_points=1000):\n  \n    data = []\n    data_labels = []\n    with open(filename) as file:\n        for line in file:\n            cols = line.split(\"\\t\")\n            label = cols[1]\n            text = cols[2]\n            \n            data.append(text)\n            data_labels.append(labels[label])\n            \n\n    # shuffle the data\n    tmp = list(zip(data, data_labels))\n    random.shuffle(tmp)\n    data, data_labels = zip(*tmp)\n    \n    if max_data_points is None:\n        return data, data_labels\n    \n    return data[:max_data_points], data_labels[:max_data_points]",
   "metadata": {
    "cell_id": "8fb1ecdc68814f2eb51e29ab17a5fc4d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "81d9901c",
    "execution_start": 1650504022384,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 477
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "labels=read_labels(\"train.txt\")\ntrain_x, train_y=read_data(\"train.txt\", labels, max_data_points=None)\ndev_x, dev_y=read_data(\"train.txt\", labels, max_data_points=None)\ntest_x, test_y=read_data(\"train.txt\", labels, max_data_points=None)",
   "metadata": {
    "cell_id": "c93cd41ea73244b8b941366eb854e904",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "50f96074",
    "execution_start": 1650504022385,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "def evaluate(model, x, y):\n    model.eval()\n    corr = 0.\n    total = 0.\n    with torch.no_grad():\n        for x, y in zip(x, y):\n            y_preds=model.forward(x)\n            for idx, y_pred in enumerate(y_preds):\n                prediction=torch.argmax(y_pred)\n                if prediction == y[idx]:\n                    corr += 1.\n                total+=1                          \n    return corr/total, total",
   "metadata": {
    "cell_id": "2dd8c86645be4e92812dd477009487bb",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c6cec94a",
    "execution_start": 1650504022385,
    "execution_millis": 2,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 297
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "class BERTClassifier(nn.Module):\n\n    def __init__(self, bert_model_name, params):\n        super().__init__()\n    \n        self.model_name=bert_model_name\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"], do_basic_tokenize=False)\n        self.bert = BertModel.from_pretrained(self.model_name)\n        \n        self.num_labels = params[\"label_length\"]\n\n        self.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n\n    def get_batches(self, all_x, all_y, batch_size=16, max_toks=510):\n            \n        \"\"\" Get batches for input x, y data, with data tokenized according to the BERT tokenizer \n      (and limited to a maximum number of WordPiece tokens \"\"\"\n\n        batches_x=[]\n        batches_y=[]\n        \n        for i in range(0, len(all_x), batch_size):\n\n            current_batch=[]\n\n            x=all_x[i:i+batch_size]\n\n            batch_x = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks)\n            batch_y=all_y[i:i+batch_size]\n\n            batches_x.append(batch_x.to(device))\n            batches_y.append(torch.LongTensor(batch_y).to(device))\n            \n        return batches_x, batches_y\n        \n    def forward(self, batch_x): \n    \n        bert_output = self.bert(input_ids=batch_x[\"input_ids\"],\n                         attention_mask=batch_x[\"attention_mask\"],\n                         token_type_ids=batch_x[\"token_type_ids\"],\n                         output_hidden_states=True)\n\n      # We're going to represent an entire document just by its [CLS] embedding (at position 0)\n      # And use the *last* layer output (layer -1)\n      # as a result of this choice, this embedding will be optimized for this purpose during the training process.\n      \n        bert_hidden_states = bert_output['hidden_states']\n\n        out = bert_hidden_states[-1][:,0,:]\n\n        out = self.fc(out)\n\n        return out.squeeze()",
   "metadata": {
    "cell_id": "dbceb31a555c4454a38f5997290c37e9",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3cbafd38",
    "execution_start": 1650504022400,
    "execution_millis": 9,
    "owner_user_id": "8b02d796-d3e3-4ac0-90c7-b5210851ec37",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1080.1875
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "def confidence_intervals(accuracy, n, significance_level):\n    critical_value=(1-significance_level)/2\n    z_alpha=-1*norm.ppf(critical_value)\n    se=math.sqrt((accuracy*(1-accuracy))/n)\n    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)",
   "metadata": {
    "cell_id": "93d4ee21d36c4f189b06f4410eb107bc",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f5d81a47",
    "execution_start": 1650504022410,
    "execution_millis": 667433,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 153
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "def train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=None):\n\n    bert_model = BERTClassifier(bert_model_name, params={\"label_length\": len(labels), \"doLowerCase\":doLowerCase, \"embedding_size\":embedding_size})\n    bert_model.to(device)\n\n    batch_x, batch_y = bert_model.get_batches(train_x, train_y)\n    dev_batch_x, dev_batch_y = bert_model.get_batches(dev_x, dev_y)\n\n    optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n    cross_entropy=nn.CrossEntropyLoss()\n\n    num_epochs=30\n    best_dev_acc = 0.\n    patience=5\n\n    best_epoch=0\n\n    for epoch in range(num_epochs):\n        bert_model.train()\n\n        # Train\n        for x, y in zip(batch_x, batch_y):\n            y_pred = bert_model.forward(x)\n            loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Evaluate\n        dev_accuracy, _=evaluate(bert_model, dev_batch_x, dev_batch_y)\n        if epoch % 1 == 0:\n            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n            if dev_accuracy > best_dev_acc:\n                torch.save(bert_model.state_dict(), model_filename)\n                best_dev_acc = dev_accuracy\n                best_epoch=epoch\n        if epoch - best_epoch > patience:\n            print(\"No improvement in dev accuracy over %s epochs; stopping training\" % patience)\n            break\n\n    bert_model.load_state_dict(torch.load(model_filename))\n    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n    return bert_model",
   "metadata": {
    "cell_id": "85df6d700d7f4680804e81088d4c1910",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "31d123c4",
    "execution_start": 1650504022456,
    "execution_millis": 667428,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 837
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "# small BERT -- can run on laptop\nbert_model_name=\"google/bert_uncased_L-2_H-128_A-2\"\nmodel_filename=\"mybert.model\"\nembedding_size=128\ndoLowerCase=True\n\n# # bert-base -- slow on laptop; better on Colab\n# bert_model_name=\"bert-base-cased\"\n# model_filename=\"mybert.model\"\n# embedding_size=768\n# doLowerCase=False\n\nmodel=train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=embedding_size, doLowerCase=doLowerCase)",
   "metadata": {
    "cell_id": "b0a698d4d47e4e889d3bf95ed6051d8d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1f1417e0",
    "execution_start": 1650504022457,
    "execution_millis": 10,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 670
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 38.4MB/s]\nDownloading: 100%|██████████| 382/382 [00:00<00:00, 205kB/s]\nDownloading: 100%|██████████| 16.9M/16.9M [00:00<00:00, 61.9MB/s]\nSome weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "test_batch_x, test_batch_y = model.get_batches(test_x, test_y)\naccuracy, test_n=evaluate(model, test_batch_x, test_batch_y)\n\nlower, upper=confidence_intervals(accuracy, test_n, .95)\nprint(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))",
   "metadata": {
    "cell_id": "6b84dee2339b462ebb99f558fbfa0f43",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "49454836",
    "execution_start": 1650503166471,
    "execution_millis": 410,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 216.1875
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bc4532dfaeeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_batch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfidence_intervals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=743801e4-5725-4df3-b423-e3fdf8e554eb' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {},
  "deepnote_notebook_id": "101d6d98-a46e-4708-a6fe-29728ef4f1b5",
  "deepnote_execution_queue": [
   {
    "cellId": "b0a698d4d47e4e889d3bf95ed6051d8d",
    "sessionId": "655e8e6d-5ddd-4953-97d3-721dda0eeadd",
    "msgId": "0cf179db-76ba-4a05-9b8e-4918d415920e"
   },
   {
    "cellId": "6b84dee2339b462ebb99f558fbfa0f43",
    "sessionId": "655e8e6d-5ddd-4953-97d3-721dda0eeadd",
    "msgId": "f9eac6c2-0cfc-45d6-af05-0229f9795147"
   }
  ]
 }
}